{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophiezhzh18/NLP/blob/main/6861_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU7xWiY6TyWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8e7d2e-2cde-4564-da1d-4e0bc48d14e6"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "rm -rf hw2\n",
        "git clone https://github.com/mit-6864/hw2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'hw2'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0MHaHrdUACZ"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw2\")\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ3MUj4iUf76"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, you will find code scaffolding for the implementation portion of Homework 1. There are certain parts of the scaffolding marked with `# Your code here!` comments where you can fill in code to perform the specified tasks. You should be able to complete this assignment without changing any of the scaffolding code, just writing code to fill in the scaffolding and run experiments. Make sure to read the text between cells for certain implementation details. Please submit the notebook with all code cells running.\n",
        "\n",
        "This notebook can be done independently of the handout and will be graded based on the code and cell outputs. However, certain questions on the handout require you to design and perform experiments to evaluate the methods used here. There is space at the end of the notebook for you to carry out these experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG654Y9J3yHw"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We're going to be working with a dataset of product reviews. The following cell loads the dataset and splits it into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwiX-Tc9V1xI",
        "collapsed": true
      },
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "with open(\"/content/hw2/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "      n_positive += 1\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "\n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "    n_disp += 1\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews, train_labels = reviews[:3000], labels[:3000]\n",
        "val_reviews, val_labels = reviews[3000:3500], labels[3000:3500]\n",
        "test_reviews, test_labels = reviews[3500:], labels[3500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries: Word-document representations\n",
        "\n",
        "We start by constructing the bag-of-words matrix (look at `/content/hw2/lab_util.py` in the file browser on the left if you want to see how this works)."
      ],
      "metadata": {
        "id": "kx9mHkiHil1N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WPt6Y7-Z_7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346feb8e-7d6a-41e7-a1a4-1dde1f38f4af"
      },
      "source": [
        "vectorizer = lab_util.CountVectorizer()\n",
        "vectorizer.fit(train_reviews)\n",
        "bow_matrix = vectorizer.transform(train_reviews)\n",
        "print(f\"BoW matrix is {bow_matrix.shape[0]} x {bow_matrix.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW matrix is 3000 x 2006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In class, we've seen that we can get more informative representations by using representations other than raw counts. Implement the TF-IDF transform below.\n",
        "\n",
        "Note: In lecture, we multiplied the raw term frequencies by idfs to get the TF-IDF matrix (tfidf=tf*idf). Feel free to experiment with other transformations, such as log(1+tf) for the measure of term frequency."
      ],
      "metadata": {
        "id": "6QWx2_CJ2AT-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y3PmW-IgpqA"
      },
      "source": [
        "class TfidfFeaturizer:\n",
        "    def fit(self, matrix):\n",
        "        # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the\n",
        "        # vocabulary size and `|D|` is the number of documents in the corpus.\n",
        "        # This function should create the inverse document frequency (idf) matrix\n",
        "        # for the given term-document matrix.\n",
        "\n",
        "        self.idf = None # Your code here!\n",
        "\n",
        "    def transform_tfidf(self, matrix):\n",
        "        # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the\n",
        "        # vocabulary size and `|D|` is the number of documents in the corpus. This\n",
        "        # function should (nondestructively) return a version of `matrix` with the\n",
        "        # TF-IDF transform applied.\n",
        "\n",
        "        # Your code here!\n",
        "        raise NotImplementedError\n",
        "\n",
        "td_matrix = bow_matrix.T\n",
        "featurizer = TfidfFeaturizer()\n",
        "featurizer.fit(td_matrix)\n",
        "tfidf_matrix = featurizer.transform_tfidf(td_matrix)\n",
        "print(f\"TF-IDF matrix is {tfidf_matrix.shape[0]} x {tfidf_matrix.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnfIOUYjEZjw"
      },
      "source": [
        "#### Sanity check 1\n",
        "The following cell should print `True` if your `transform_tfidf` function is implemented properly. (*Hint: in our implementation, we use the natural logarithm (base $e$) when computing inverse document frequency.*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVtphNeDEj2W"
      },
      "source": [
        "DEBUG_sc1_matrix = np.array([[3,1,0,3,0],\n",
        "                             [0,2,0,0,1],\n",
        "                             [7,8,2,0,1],\n",
        "                             [1,9,8,1,0]])\n",
        "DEBUG_gt = np.array([[1.53247687, 0.51082562, 0.        , 1.53247687, 0.        ],\n",
        "                     [0.        , 1.83258146, 0.        , 0.        , 0.91629073],\n",
        "                     [1.56200486, 1.78514841, 0.4462871 , 0.        , 0.22314355],\n",
        "                     [0.22314355, 2.00829196, 1.78514841, 0.22314355, 0.        ]])\n",
        "debug = TfidfFeaturizer()\n",
        "debug.fit(DEBUG_sc1_matrix)\n",
        "print(np.allclose(debug.transform_tfidf(DEBUG_sc1_matrix), DEBUG_gt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear models on BoW and TFIDF features\n",
        "\n",
        "Now we have two feature representations, BoW and TF-IDF. Let's first see how effective these features are for the sentiment classification task.\n",
        "\n",
        "Below, implement two logistic regression models to classify the reviews, using BoW and TF-IDF respectively. You should feel free to use the `scikit-learn` library, which has the `sklearn.linear_model.LogisticRegression` available for you. Report the training and test accuracy of these two models.\n",
        "\n",
        "Note: For the TF-IDF classifier, we only fit the IDF matrix to the training data. (Think about why you might not want a separate IDF for the test set!)\n"
      ],
      "metadata": {
        "id": "Q1RKNTu-voxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_and_eval(train_X, train_y, test_X, test_y):\n",
        "    # Create and train a model that takes as input a feature\n",
        "    # representation of the training data and outputs a sentiment label.\n",
        "    # Make sure to report the training and test accuracy of your model.\n",
        "    # Hint: changing max_iter might be helpful\n",
        "\n",
        "    # Your code here!\n",
        "    raise NotImplementedError\n",
        "\n",
        "print('Logistic regression with bag of word features')\n",
        "# Your code here!\n",
        "\n",
        "print('Logistic regression with tf-idf features')\n",
        "# Your code here!"
      ],
      "metadata": {
        "id": "VWcucgqlzGwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what the model learns about sentiment. For both models, display the top 5 most positive and negative weights, as well as their corresponding words.\n",
        "\n",
        "Hint: look at `/content/hw2/lab_util.py` for how to convert between token indices and words."
      ],
      "metadata": {
        "id": "bXbP130wDNwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here!"
      ],
      "metadata": {
        "id": "Jn89QR_3DMM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twLHWqM6Z5xD"
      },
      "source": [
        "# LSA: Word representations via matrix factorization\n",
        "\n",
        "In class, we've seen that the above approaches can lead to high dimensional representations. To alleviate this, we can use latent semantic analysis (LSA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd3-pw4XbD4B"
      },
      "source": [
        "First, implement the function `learn_reps_lsa` that computes word representations via latent semantic analysis. The `sklearn.decomposition` or `np.linalg` packages may be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASVs8KubeBE"
      },
      "source": [
        "def learn_reps_lsa(matrix, rep_size):\n",
        "    # `matrix` is a `|V| x |D|` matrix, where `|V|` is the number of words in the\n",
        "    # vocabulary and |D| is the number of training reviews. This function should\n",
        "    # return a `|V| x rep_size` matrix with each row corresponding to a word\n",
        "    # representation. The `sklearn.decomposition` package may be useful.\n",
        "\n",
        "    # Your code here!\n",
        "    raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fxv02wO9LbS"
      },
      "source": [
        "#### Sanity check 2\n",
        "The following cell contains a simple sanity check for your `learn_reps_lsa` implementation: it should print `True` if your `learn_reps_lsa` function is implemented equivalently to one of our solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsRlKB9q9Js-"
      },
      "source": [
        "DEBUG_sc2_matrix = np.array([[1,0,0,2,1,3,5],\n",
        "                             [2,0,0,0,0,4,0],\n",
        "                             [0,3,4,1,8,6,6],\n",
        "                             [1,4,5,0,0,0,0]])\n",
        "\n",
        "DEBUG_reps = learn_reps_lsa(DEBUG_sc2_matrix, 3)\n",
        "DEBUG_gt1 = np.array([[ -4.92017554,  -2.85465774,   1.18575453],\n",
        "                      [ -2.14977584,  -1.19987977,   3.37221899],\n",
        "                      [-12.62664695,   0.10890093,  -1.32131745],\n",
        "                      [ -2.69216011,   5.66453534,   1.33728063]])\n",
        "DEBUG_gt2 = np.array([[-0.35188159, -0.44213061,  0.29358929],\n",
        "                      [-0.15374788, -0.18583789,  0.83495136],\n",
        "                      [-0.90303377,  0.01686662, -0.32715426],\n",
        "                      [-0.19253817,  0.87732566,  0.3311067 ]])\n",
        "\n",
        "print(np.allclose(np.abs(DEBUG_reps), np.abs(DEBUG_gt1)) or np.allclose(np.abs(DEBUG_reps), np.abs(DEBUG_gt2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKWzRC0dclVK"
      },
      "source": [
        "Let's look at some representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ad7RZkwceWw"
      },
      "source": [
        "# LSA reps for term-document matrix\n",
        "# Feel free to change the rep size!\n",
        "reps = learn_reps_lsa(td_matrix, 500)\n",
        "words = [\"good\", \"bad\", \"cookie\", \"jelly\", \"dog\", \"the\", \"3\"]\n",
        "show_tokens = [vectorizer.tokenizer.word_to_token[word] for word in words]\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps, show_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do the given similar words change if we apply LSA to the TF-IDF matrix instead?"
      ],
      "metadata": {
        "id": "obvh2vusBGpJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV5xKLYTi7LA"
      },
      "source": [
        "# Feel free to change the rep size!\n",
        "reps_tfidf = learn_reps_lsa(tfidf_matrix, 500)\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_tfidf, show_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO-NG4u1kG9z"
      },
      "source": [
        "Now that we have some representations, let's see if we can do something useful with them.\n",
        "\n",
        "Below, implement a feature function that represents a document as the sum of its\n",
        "learned word embeddings.\n",
        "\n",
        "The remaining code trains a logistic regression model on a set of *labeled* reviews; we're interested in seeing how much representations learned from *unlabeled* reviews, or reviews without any additional human annotation, can improve classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B08xvIFlee3"
      },
      "source": [
        "import sklearn.linear_model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def word_featurizer(xs):\n",
        "    # normalize\n",
        "    return xs / np.sqrt((xs ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "def lsa_featurizer(xs):\n",
        "    # This function takes in a matrix in which each row contains the word counts\n",
        "    # for the given review. It should return a matrix in which each row contains\n",
        "    # the learned feature representation of each review (e.g. the sum of LSA\n",
        "    # word representations).\n",
        "\n",
        "    feats = None # Your code here!\n",
        "\n",
        "    # normalize\n",
        "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "def combo_featurizer(xs):\n",
        "    return np.concatenate((word_featurizer(xs), lsa_featurizer(xs)), axis=1)\n",
        "\n",
        "def train_model(featurizer, xs, ys):\n",
        "    xs_featurized = featurizer(xs)\n",
        "    model = sklearn.linear_model.LogisticRegression(penalty=None, max_iter=1000)\n",
        "    model.fit(xs_featurized, ys)\n",
        "    return model\n",
        "\n",
        "def eval_model(model, featurizer, xs, ys):\n",
        "    xs_featurized = featurizer(xs)\n",
        "    pred_ys = model.predict(xs_featurized)\n",
        "    return np.mean(pred_ys == ys)\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "    print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = vectorizer.transform(train_reviews[:n_train])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = vectorizer.transform(test_reviews)\n",
        "    test_ys = test_labels\n",
        "    model = train_model(featurizer, train_xs, train_ys)\n",
        "    acc = eval_model(model, featurizer, test_xs, test_ys)\n",
        "    print(acc, '\\n')\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxx5_aSR67h9"
      },
      "source": [
        "# this will run a training experiment with all 3k examples in training set\n",
        "n_train = 3000\n",
        "training_experiment(\"word\", word_featurizer, n_train)\n",
        "training_experiment(\"lsa\", lsa_featurizer, n_train)\n",
        "training_experiment(\"combo\", combo_featurizer, n_train)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxfunCYh5nmZ"
      },
      "source": [
        "# Word2vec: word representations via neural model\n",
        "\n",
        "In this section, we'll train a word embedding model with a word2vec-style objective rather than a matrix factorization objective. This requires a little more work; we've provided scaffolding for a PyTorch model implementation below.\n",
        "If you don't have much PyTorch experience, there are some tutorials [here](https://pytorch.org/tutorials/) which may be useful. You may also find the classes `nn.Embedding` and `nn.EmbeddingBag` useful.\n",
        "\n",
        "Note: We will be implementing a CBOW model; that is, given a word's context, we will predict the central word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1napibQ6aub"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "class Word2VecModel(nn.Module):\n",
        "    # A torch module implementing a word2vec predictor. The `forward` function\n",
        "    # should take a batch of context word ids as input and predict the word\n",
        "    # in the middle of the context as output, as in the CBOW model from lecture.\n",
        "    # Hint: look at how padding is handled in lab_util.get_ngrams when\n",
        "    # initializing `ctx`: vocab_size is used as the padding token for contexts\n",
        "    # near the beginning and end of sequences. If you use an embedding module\n",
        "    # in your Word2Vec implementation, make sure to account for this extra\n",
        "    # padding token in the input dimension and include the `padding_idx` kwarg.\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, padding_idx=2006):\n",
        "        super().__init__()\n",
        "\n",
        "        # Your code here!\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, context):\n",
        "        # Context is an `n_batch x n_context` matrix of integer word ids.\n",
        "        # In this case, n_context = 2 * window_size where window_size is defined\n",
        "        # in lab_util.py. This is because each word has both left and right context.\n",
        "        # This function should return an `n_batch x vocab_size` matrix with\n",
        "        # element i, j being the (possibly log) probability of the middle word\n",
        "        # in context i being word j.\n",
        "\n",
        "        # Your code here!\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model using the function below. Note that we use an [Adam optimizer](https://arxiv.org/abs/1412.6980). This is a fancy version of SGD which uses momentum and adaptive updates.\n",
        "\n"
      ],
      "metadata": {
        "id": "FOE4RyP9LX26"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePgZlityuWr3"
      },
      "source": [
        "def learn_reps_word2vec(corpus, window_size, rep_size, n_epochs, n_batch):\n",
        "    # This method takes in a corpus of training sentences. It returns a matrix of\n",
        "    # word embeddings with the same structure as used in the previous section of\n",
        "    # the assignment. (You can extract this matrix from the parameters of the\n",
        "    # Word2VecModel.)\n",
        "\n",
        "    tokenizer = lab_util.Tokenizer()\n",
        "    tokenizer.fit(corpus)\n",
        "    tokenized_corpus = tokenizer.tokenize(corpus)\n",
        "\n",
        "    ngrams = lab_util.get_ngrams(tokenized_corpus, window_size, pad_idx=2006)\n",
        "\n",
        "    device = torch.device('cuda')  # run on colab gpu\n",
        "    model = Word2VecModel(tokenizer.vocab_size, rep_size).to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    loader = torch_data.DataLoader(ngrams, batch_size=n_batch, shuffle=True)\n",
        "\n",
        "    # What loss function should we use for Word2Vec?\n",
        "    loss_fn = None  # Your code here!\n",
        "\n",
        "    losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        epoch_loss = 0\n",
        "        for context, label in loader:\n",
        "            # As described above, `context` is a batch of context word ids, and\n",
        "            # `label` is a batch of predicted word labels.\n",
        "\n",
        "            # Here, perform a forward pass to compute predictions for the model.\n",
        "            # Your code here!\n",
        "            preds = None\n",
        "\n",
        "\n",
        "            # Now finish the backward pass and gradient update.\n",
        "            # Remember, you need to compute the loss, zero the gradie nts\n",
        "            # of the model parameters, perform the backward pass, and\n",
        "            # update the model parameters.\n",
        "            # Your code here!\n",
        "            loss = None\n",
        "\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "    # Hint: you want to return a `vocab_size x embedding_size` numpy array\n",
        "    embedding_matrix = None  # Your code here!\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaUy1cNuB3W1"
      },
      "source": [
        "# Feel free to change the hyperparameters!\n",
        "# Use the function you just wrote to learn Word2Vec embeddings:\n",
        "reps_word2vec = learn_reps_word2vec(train_reviews, 2, 500, 10, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3oE-tpR7I39"
      },
      "source": [
        "After training the embeddings, we can try to visualize the embedding space to see if it makes sense. First, we can take any word in the space and check its closest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMW4QND56bHF"
      },
      "source": [
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_word2vec, show_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-9CPSc7fi9"
      },
      "source": [
        "We can also cluster the embedding space. Clustering in 4 or more dimensions is hard to visualize, and even clustering in 2 or 3 can be difficult because there are so many words in the vocabulary. One thing we can try to do is assign cluster labels and qualitiatively look for an underlying pattern in the clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Yf6NMCXVx4"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "indices = KMeans(n_clusters=10).fit_predict(reps_word2vec)\n",
        "zipped = list(zip(range(vectorizer.tokenizer.vocab_size), indices))\n",
        "np.random.shuffle(zipped)\n",
        "zipped = zipped[:100]\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for token, cluster_idx in zipped:\n",
        "  word = vectorizer.tokenizer.token_to_word[token]\n",
        "  print(f\"{word}: {cluster_idx}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1TkENU78Wn"
      },
      "source": [
        "Finally, we can use the trained word embeddings to construct vector representations of full reviews. One common approach is to simply average all the word embeddings in the review to create an overall embedding. Implement the transform function in Word2VecFeaturizer to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5vjmRV6Dgbu"
      },
      "source": [
        "def w2v_featurizer(xs):\n",
        "    # This function takes in a matrix in which each row contains the word counts\n",
        "    # for the given review. It should return a matrix in which each row contains\n",
        "    # the average Word2Vec embedding of each review (hint: this will be very\n",
        "    # similar to `lsa_featurizer` from above, just using Word2Vec embeddings\n",
        "    # instead of LSA).\n",
        "\n",
        "    feats = None # Your code here!\n",
        "\n",
        "    # normalize\n",
        "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "training_experiment(\"word2vec\", w2v_featurizer, 3000)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments for HW1\n",
        "\n",
        "Below, you can implement experiments to answer the experimental questions in the HW1 handout. Please label each code cell with its relevant question part."
      ],
      "metadata": {
        "id": "XB1nze9Jnmgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here!"
      ],
      "metadata": {
        "id": "l9u7BNHuoB9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}